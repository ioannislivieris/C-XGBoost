{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.chdir(\"../\")\n",
    "import random\n",
    "import xgboost\n",
    "import time\n",
    "import numpy as np\n",
    "from utils.metrics import PEHE, ATE\n",
    "\n",
    "# Random generators initialization\n",
    "seed=42\n",
    "random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataset imported\n",
      "[INFO] Number of training instances: 5000\n",
      "[INFO] Number of testing instances: 1000\n",
      "[0]\tvalidation_0-rmse:0.49688\tvalidation_1-rmse:0.49679\n",
      "[25]\tvalidation_0-rmse:0.43347\tvalidation_1-rmse:0.43116\n",
      "[50]\tvalidation_0-rmse:0.39287\tvalidation_1-rmse:0.38861\n",
      "[75]\tvalidation_0-rmse:0.36818\tvalidation_1-rmse:0.36219\n",
      "[100]\tvalidation_0-rmse:0.35408\tvalidation_1-rmse:0.34675\n",
      "[125]\tvalidation_0-rmse:0.34655\tvalidation_1-rmse:0.33828\n",
      "[150]\tvalidation_0-rmse:0.34292\tvalidation_1-rmse:0.33400\n",
      "[175]\tvalidation_0-rmse:0.34150\tvalidation_1-rmse:0.33218\n",
      "[200]\tvalidation_0-rmse:0.34122\tvalidation_1-rmse:0.33175\n",
      "[225]\tvalidation_0-rmse:0.34154\tvalidation_1-rmse:0.33203\n",
      "[250]\tvalidation_0-rmse:0.34203\tvalidation_1-rmse:0.33264\n",
      "[275]\tvalidation_0-rmse:0.34249\tvalidation_1-rmse:0.33332\n",
      "[300]\tvalidation_0-rmse:0.34295\tvalidation_1-rmse:0.33403\n",
      "[325]\tvalidation_0-rmse:0.34335\tvalidation_1-rmse:0.33469\n",
      "[350]\tvalidation_0-rmse:0.34359\tvalidation_1-rmse:0.33528\n",
      "[375]\tvalidation_0-rmse:0.34372\tvalidation_1-rmse:0.33584\n",
      "[400]\tvalidation_0-rmse:0.34385\tvalidation_1-rmse:0.33630\n",
      "[425]\tvalidation_0-rmse:0.34396\tvalidation_1-rmse:0.33667\n",
      "[450]\tvalidation_0-rmse:0.34394\tvalidation_1-rmse:0.33692\n",
      "[475]\tvalidation_0-rmse:0.34389\tvalidation_1-rmse:0.33728\n",
      "[499]\tvalidation_0-rmse:0.34378\tvalidation_1-rmse:0.33756\n",
      "[INFO] Model trained\n",
      "[INFO] Time 5.22 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start timer\n",
    "start = time.time()\n",
    "\n",
    "# Load train data\n",
    "data = np.load('Data/train.npz')\n",
    "trainX, trainT, trainY, train_potential_Y = data['X'], data['T'], data['Y'], data['potential_Y']\n",
    "# Load valid data\n",
    "data = np.load('Data/test.npz')\n",
    "testX, testT, testY, test_potential_Y = data['X'], data['T'], data['Y'], data['potential_Y']\n",
    "print(\"[INFO] Dataset imported\")\n",
    "print(f\"[INFO] Number of training instances: {trainX.shape[0]}\")\n",
    "print(f\"[INFO] Number of testing instances: {testX.shape[0]}\")\n",
    "\n",
    "    \n",
    "\n",
    "# Loss function\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "treatment = np.array([[1,0] if x == 0 else [0,1] for x in trainT]).flatten()\n",
    "\n",
    "def custom_loss(y_true:np.ndarray=None, y_pred:np.ndarray=None)->(np.ndarray, np.ndarray):\n",
    "    grad = 2*(y_pred.flatten() - y_true.flatten()) * treatment\n",
    "    hess = (0*y_pred.flatten() + 2)  * treatment\n",
    "\n",
    "    return grad, hess\n",
    "    \n",
    "\n",
    "\n",
    "# Setup XGBoost\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "model = xgboost.XGBRegressor(n_estimators=500, \n",
    "                                max_depth=4, \n",
    "                                objective=custom_loss, \n",
    "                                learning_rate=1e-2, \n",
    "                            #  booster=\"gblinear\",\n",
    "                                n_jobs=-1,\n",
    "                                tree_method=\"hist\", \n",
    "                                multi_strategy=\"multi_output_tree\")\n",
    "    \n",
    "# Create outputs for DragonNet (concatenate Y & T)\n",
    "yt_train = np.concatenate([trainY.reshape(-1,1), trainT.reshape(-1,1)], axis = 1)\n",
    "\n",
    "\n",
    "# Train model\n",
    "model.fit(trainX, yt_train, eval_set=[(trainX, train_potential_Y), (testX, test_potential_Y)], verbose=25);\n",
    "print('[INFO] Model trained')    \n",
    "print('[INFO] Time %.2f seconds' % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error_ATE: 0.095\n",
      "PEHE: 0.230\n"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "test_y_hat = model.predict(testX)\n",
    "\n",
    "# Calculate performance metrics\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "# ATE\n",
    "real_ATE = ( test_potential_Y[:,1] - test_potential_Y[:,0] ).mean()      \n",
    "# Error ATE\n",
    "Error_ATE = ATE(test_potential_Y, test_y_hat)  \n",
    "# PEHE\n",
    "PEHE_score = PEHE(test_potential_Y, test_y_hat)\n",
    "\n",
    "print(f\"Error_ATE: {Error_ATE:.3f}\")\n",
    "print(f\"PEHE: {PEHE_score:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
